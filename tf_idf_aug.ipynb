{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1747602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import nlpaug.augmenter.word.context_word_embs as aug\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa199371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7115, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this article is about the herbivorous mammals....</td>\n",
       "      <td>Antelope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>one new world species, the pronghorn of north ...</td>\n",
       "      <td>Antelope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the english word \"animal\" first appeared in 14...</td>\n",
       "      <td>Antelope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the word talopus and calopus, from latin, came...</td>\n",
       "      <td>Antelope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>animal are not a cladistic or taxonomically de...</td>\n",
       "      <td>Antelope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text     class\n",
       "0           0  this article is about the herbivorous mammals....  Antelope\n",
       "1           1  one new world species, the pronghorn of north ...  Antelope\n",
       "2           2  the english word \"animal\" first appeared in 14...  Antelope\n",
       "3           3  the word talopus and calopus, from latin, came...  Antelope\n",
       "4           4  animal are not a cladistic or taxonomically de...  Antelope"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data into a pandas dataframe\n",
    "df = pd.read_csv(\"bert_data_withoutClassWord.csv\")\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105e94bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>class_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this article is about the herbivorous mammals....</td>\n",
       "      <td>Antelope</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>one new world species, the pronghorn of north ...</td>\n",
       "      <td>Antelope</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the english word \"animal\" first appeared in 14...</td>\n",
       "      <td>Antelope</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the word talopus and calopus, from latin, came...</td>\n",
       "      <td>Antelope</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>animal are not a cladistic or taxonomically de...</td>\n",
       "      <td>Antelope</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7110</th>\n",
       "      <td>7110</td>\n",
       "      <td>technology to use sponges as mouth protection ...</td>\n",
       "      <td>dolphin</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7111</th>\n",
       "      <td>7111</td>\n",
       "      <td>pesticides, heavy metals, plastics, and other ...</td>\n",
       "      <td>dolphin</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7112</th>\n",
       "      <td>7112</td>\n",
       "      <td>hundreds of orcas, animals and other members o...</td>\n",
       "      <td>dolphin</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>7113</td>\n",
       "      <td>captured orcas and animals are confined to tan...</td>\n",
       "      <td>dolphin</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7114</th>\n",
       "      <td>7114</td>\n",
       "      <td>marine parks may withhold up to 60 percent of ...</td>\n",
       "      <td>dolphin</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7115 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text     class  \\\n",
       "0              0  this article is about the herbivorous mammals....  Antelope   \n",
       "1              1  one new world species, the pronghorn of north ...  Antelope   \n",
       "2              2  the english word \"animal\" first appeared in 14...  Antelope   \n",
       "3              3  the word talopus and calopus, from latin, came...  Antelope   \n",
       "4              4  animal are not a cladistic or taxonomically de...  Antelope   \n",
       "...          ...                                                ...       ...   \n",
       "7110        7110  technology to use sponges as mouth protection ...   dolphin   \n",
       "7111        7111  pesticides, heavy metals, plastics, and other ...   dolphin   \n",
       "7112        7112  hundreds of orcas, animals and other members o...   dolphin   \n",
       "7113        7113  captured orcas and animals are confined to tan...   dolphin   \n",
       "7114        7114  marine parks may withhold up to 60 percent of ...   dolphin   \n",
       "\n",
       "      class_num  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "7110       49.0  \n",
       "7111       49.0  \n",
       "7112       49.0  \n",
       "7113       49.0  \n",
       "7114       49.0  \n",
       "\n",
       "[7115 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the new column which gives a unique number to each of these labels \n",
    "\n",
    "j = 0\n",
    "for i in df['class'].unique():\n",
    "    df.loc[df['class'] == i, ['class_num']] = j\n",
    "    j += 1\n",
    "\n",
    "#checking the results \n",
    "df.head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc496ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.text, \n",
    "    df.class_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df.class_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b681cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embbeding_size = 5000\n",
    "clustering_count = 3\n",
    "numof_embb = 2\n",
    "att_size = embbeding_size * clustering_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83dea093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.39      0.33        38\n",
      "         1.0       0.42      0.35      0.38        23\n",
      "         2.0       0.68      0.54      0.60        28\n",
      "         3.0       0.39      0.54      0.45        37\n",
      "         4.0       0.31      0.17      0.22        23\n",
      "         5.0       0.29      0.54      0.37        37\n",
      "         6.0       0.36      0.43      0.39        44\n",
      "         7.0       0.38      0.58      0.46        31\n",
      "         8.0       0.46      0.48      0.47        23\n",
      "         9.0       0.50      0.36      0.42        25\n",
      "        10.0       0.60      0.29      0.39        21\n",
      "        11.0       0.42      0.25      0.31        20\n",
      "        12.0       0.34      0.28      0.31        36\n",
      "        13.0       0.73      0.66      0.69        29\n",
      "        14.0       0.65      0.38      0.48        29\n",
      "        15.0       0.54      0.52      0.53        27\n",
      "        16.0       0.50      0.17      0.25        12\n",
      "        17.0       0.27      0.14      0.19        21\n",
      "        18.0       0.40      0.46      0.43        39\n",
      "        19.0       0.53      0.59      0.56        32\n",
      "        20.0       1.00      0.50      0.67        12\n",
      "        21.0       0.36      0.76      0.48        21\n",
      "        22.0       0.63      0.55      0.59        31\n",
      "        23.0       0.62      0.69      0.65        45\n",
      "        24.0       0.39      0.42      0.41        26\n",
      "        25.0       0.50      0.36      0.42        22\n",
      "        26.0       0.35      0.33      0.34        21\n",
      "        27.0       0.81      0.71      0.76        24\n",
      "        28.0       0.25      0.38      0.30        37\n",
      "        29.0       0.65      0.76      0.70        41\n",
      "        30.0       0.42      0.36      0.38        28\n",
      "        31.0       0.77      0.77      0.77        39\n",
      "        32.0       0.33      0.19      0.24        26\n",
      "        33.0       0.32      0.29      0.31        31\n",
      "        34.0       0.64      0.47      0.54        15\n",
      "        35.0       0.56      0.42      0.48        24\n",
      "        36.0       0.80      0.62      0.70        26\n",
      "        37.0       0.61      0.61      0.61        31\n",
      "        38.0       0.55      0.73      0.62        33\n",
      "        39.0       0.47      0.79      0.59        33\n",
      "        40.0       0.54      0.48      0.51        29\n",
      "        41.0       0.38      0.25      0.30        20\n",
      "        42.0       0.51      0.54      0.53        37\n",
      "        43.0       0.88      0.68      0.77        22\n",
      "        44.0       0.41      0.40      0.41        35\n",
      "        45.0       0.64      0.36      0.46        25\n",
      "        46.0       0.93      0.56      0.70        25\n",
      "        47.0       0.39      0.32      0.35        28\n",
      "        48.0       0.70      0.57      0.63        28\n",
      "        49.0       0.57      0.48      0.52        33\n",
      "\n",
      "    accuracy                           0.49      1423\n",
      "   macro avg       0.52      0.47      0.48      1423\n",
      "weighted avg       0.51      0.49      0.49      1423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = embbeding_size).fit(X_train)\n",
    "        \n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "    \n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "predictions = clf.predict(vectorizer.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45ffedde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Missed torch library. Install torch by following https://pytorch.org/get-started/locally/`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\piptext\\lib\\site-packages\\nlpaug\\model\\lang_models\\language_models.py:19\u001b[0m, in \u001b[0;36mLanguageModels.__init__\u001b[1;34m(self, device, model_type, temperature, top_k, top_p, batch_size, optimize, silence)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m augmenter \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mContextualWordEmbsAug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piptext\\lib\\site-packages\\nlpaug\\augmenter\\word\\context_word_embs.py:98\u001b[0m, in \u001b[0;36mContextualWordEmbsAug.__init__\u001b[1;34m(self, model_path, model_type, action, top_k, name, aug_min, aug_max, aug_p, stopwords, batch_size, device, force_reload, stopwords_regex, verbose, silence, use_custom_api)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# TODO: Slow when switching to HuggingFace pipeline. #https://github.com/makcedward/nlpaug/issues/248\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_custom_api \u001b[38;5;241m=\u001b[39m use_custom_api\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_custom_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_custom_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Override stopwords\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# if stopwords and self.model_type in ['xlnet', 'roberta']:\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m#     stopwords = [self.stopwords]\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# lower case all stopwords\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stopwords \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muncased\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_path:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piptext\\lib\\site-packages\\nlpaug\\augmenter\\word\\context_word_embs.py:533\u001b[0m, in \u001b[0;36mContextualWordEmbsAug.get_model\u001b[1;34m(cls, model_path, model_type, device, force_reload, batch_size, top_k, silence, use_custom_api)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_path, model_type, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m    532\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_custom_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_context_word_embs_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_custom_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piptext\\lib\\site-packages\\nlpaug\\augmenter\\word\\context_word_embs.py:34\u001b[0m, in \u001b[0;36minit_context_word_embs_model\u001b[1;34m(model_path, model_type, device, force_reload, batch_size, top_k, silence, use_custom_api)\u001b[0m\n\u001b[0;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m nml\u001b[38;5;241m.\u001b[39mRoberta(model_path, device\u001b[38;5;241m=\u001b[39mdevice, top_k\u001b[38;5;241m=\u001b[39mtop_k, silence\u001b[38;5;241m=\u001b[39msilence, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mnml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel type value is unexpected. Only support bert and roberta models.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piptext\\lib\\site-packages\\nlpaug\\model\\lang_models\\bert.py:25\u001b[0m, in \u001b[0;36mBert.__init__\u001b[1;34m(self, model_path, temperature, top_k, top_p, batch_size, device, silence)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[0;32m     24\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, silence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM, AutoTokenizer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piptext\\lib\\site-packages\\nlpaug\\model\\lang_models\\language_models.py:21\u001b[0m, in \u001b[0;36mLanguageModels.__init__\u001b[1;34m(self, device, model_type, temperature, top_k, top_p, batch_size, optimize, silence)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissed torch library. Install torch by following https://pytorch.org/get-started/locally/`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# self.device = 'cuda' if device is None and torch.cuda.is_available() else 'cpu'\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Missed torch library. Install torch by following https://pytorch.org/get-started/locally/`"
     ]
    }
   ],
   "source": [
    "augmenter = aug.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b8148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piptext",
   "language": "python",
   "name": "piptext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
